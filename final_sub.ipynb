{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale, RobustScaler\n",
    "import gc\n",
    "from skimage.restoration import denoise_wavelet\n",
    "import scipy.signal as signal\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import itertools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "pd.set_option(\"max_rows\", 200)\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_cor(y_true, y_pred):\n",
    "    assert y_true.shape[0] == y_pred.shape[0]\n",
    "    \n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    numerator = (tp * tn - fp * fn) \n",
    "    denominator = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** .5\n",
    "\n",
    "    return numerator / (denominator + 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    train_metadata = pd.read_csv('../input/metadata_train.csv')\n",
    "    test_metadata = pd.read_csv('../input/metadata_test.csv')\n",
    "    return (train_metadata, test_metadata)\n",
    "\n",
    "def resample_train():\n",
    "    data = pq.read_pandas('../input/train.parquet').to_pandas().transpose()\n",
    "    target = read_metadata()[0]['target'].values\n",
    "    data['target'] = target\n",
    "    p_indices = data[data.target == 0].index\n",
    "    np.random.seed(311)\n",
    "    random_indices = np.random.choice(p_indices, 1777, replace=False)\n",
    "    df = pd.concat([data.loc[random_indices][['target']], \n",
    "                    data[target == 1][['target']]]).sample(frac=1.0, random_state=311)\n",
    "    df.to_csv('train_us_target.csv', index=False)\n",
    "    return df.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_limits = []\n",
    "start = 0\n",
    "end = 8712\n",
    "while True:\n",
    "    if (start+1000) <= 8712:\n",
    "        tr_limits.append((start, start+1000))\n",
    "        start=start+1000\n",
    "    else:\n",
    "        tr_limits.append((start, end))\n",
    "        break\n",
    "        \n",
    "ts_limits = []\n",
    "start = 8712\n",
    "end = 29049\n",
    "while True:\n",
    "    if (start+1000) <= 29049:\n",
    "        ts_limits.append((start, start+1000))\n",
    "        start=start+1000\n",
    "    else:\n",
    "        ts_limits.append((start, end))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata, test_metadata = read_metadata()\n",
    "# resampling_index = resample_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_v1 = pd.read_csv('final_v1_tr.csv')\n",
    "tr_v2 = pd.read_csv('final_v2_trs.csv')\n",
    "# tr_v3 = pd.read_csv('final_v3_tr.csv')\n",
    "# tr_v4 = pd.read_csv('final_v4_tr.csv')\n",
    "\n",
    "ts_v1 = pd.read_csv('final_v1_ts.csv')\n",
    "ts_v2 = pd.read_csv('final_v2_tss.csv')\n",
    "# ts_v3 = pd.read_csv('final_v3_ts.csv')\n",
    "# ts_v4 = pd.read_csv('final_v4_ts.csv')\n",
    "\n",
    "tr_v2.drop(['std_peak_width', 'skew_peak_width', 'std_peak_prom', 'skew_peak_prom'], axis=1, inplace=True)\n",
    "ts_v2.drop(['std_peak_width', 'skew_peak_width', 'std_peak_prom', 'skew_peak_prom'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_v3 = tr_v1 - tr_v2\n",
    "ts_v3 = ts_v1 - ts_v2\n",
    "feat_names = ['h_bt_8_dist_bt_5', 'h_bt_8_dist_bt_1000', 'height_more_15', 'height_more_8', 'h_bt_3_w_bt_5', \n",
    "              'threshold_more_16', 'h_bt_5_dist_bt_5', 'h_bt_8_dist_bt_11111', 'sg_skew', \n",
    "              'h_bt_5_dist_bt_11111', 'h_bt_8_dist_bt_7', 'max_peak_prom', 'sg_min', 'h_bt_3_dist_bt_11111', \n",
    "              'sg_mean', 'sg_std', 'h_bt_5_dist_bt_75', 'h_bt_3_dist_bt_1000', 'h_bt_3_dist_bt_75', \n",
    "              'min_peak_prom', 'max_peak_width', 'h_bt_3_dist_bt_7', 'sg_max', 'h_bt_3_w_bt_10', \n",
    "              'height_more_3', 'h_bt_3_dist_bt_25', 'h_bt_5_dist_bt_111', 'min_peak_width', \n",
    "              'threshold_more_3', 'height_more_5', 'h_bt_5_dist_bt_1000', 'threshold_more_10', \n",
    "              'h_bt_5_dist_bt_25', 'h_bt_10_dist_bt_1000']\n",
    "\n",
    "tr_v1 = tr_v1[feat_names]\n",
    "ts_v1 = ts_v1[feat_names]\n",
    "tr_v2 = tr_v2[feat_names]\n",
    "ts_v2 = ts_v2[feat_names]\n",
    "tr_v3 = tr_v3[feat_names]\n",
    "ts_v3 = ts_v3[feat_names]\n",
    "\n",
    "tr_v2.columns = ['ng-df_' + x for x in tr_v2.columns.values]\n",
    "# tr_v3.columns = ['abs-df_' + x for x in tr_v3.columns.values]\n",
    "# tr_v4.columns = ['tsfresh_' + x for x in tr_v4.columns.values]\n",
    "tr_v3.columns = ['sub-df_' + x for x in tr_v3.columns.values]\n",
    "ts_v3.columns = ['sub-df_' + x for x in ts_v3.columns.values]\n",
    "\n",
    "ts_v2.columns = ['ng-df_' + x for x in ts_v2.columns.values]\n",
    "# ts_v3.columns = ['abs-df_' + x for x in ts_v3.columns.values]\n",
    "# ts_v4.columns = ['tsfresh_' + x for x in ts_v4.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_v3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_list = [tr_v1, tr_v2, tr_v3]#, tr_v4]\n",
    "ts_list = [ts_v1, ts_v2, ts_v3]#, ts_v4]\n",
    "\n",
    "for df in tr_list+ts_list:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.concat(tr_list, axis=1)\n",
    "ts = pd.concat(ts_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly_feat = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "# tr_poly = poly_feat.fit_transform(tr)\n",
    "# ts_poly = poly_feat.fit_transform(ts)\n",
    "# tr_poly = pd.DataFrame(tr_poly)\n",
    "# ts_poly = pd.DataFrame(ts_poly)\n",
    "# print(tr_poly.shape, ts_poly.shape)\n",
    "\n",
    "# tr_sub = tr_v1 - tr_v2\n",
    "# ts_sub = ts_v1 - ts_v2   \n",
    "# print(tr_sub.shape, ts_sub.shape)\n",
    "\n",
    "# tr = pd.concat([tr_poly, tr_sub], axis=1)\n",
    "# ts = pd.concat([ts_poly, ts_sub], axis=1)\n",
    "# print(tr.shape, ts.shape)\n",
    "# del tr_poly, ts_poly, tr_sub, ts_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return ('matthews', mat_cor(labels, preds), True)\n",
    "\n",
    "train_data = lgb.Dataset(tr, label=train_metadata['target'])\n",
    "params={'learning_rate': 0.1, 'objective':'binary', 'metric':'None', \n",
    "        'num_leaves': 777, 'verbose': 1, 'seed':311, 'max_depth': 11,\n",
    "        'bagging_fraction': 0.7, 'feature_fraction': 1.0, \n",
    "        'feature_fraction_seed': 311, 'min_data_in_leaf': 33, \n",
    "        'is_unbalance': True}#, 'lambda_l1': 500, 'histogram_pool_size': 6000}\n",
    "num_round = 15000\n",
    "light = lgb.train(params, train_data, num_round, feval=evalerror)\n",
    "light_pred = light.predict(ts)\n",
    "\n",
    "light_pred = np.zeros(ts.shape[0])\n",
    "for i in range(1,4):\n",
    "    params['bagging_fraction'] = 1.0 - (i/10)\n",
    "    params['seed'] = i*110\n",
    "    params['learning_rate'] = 0.03 * i + 0.01\n",
    "    params['max_depth'] = 9 + i*2\n",
    "    params['num_leaves'] = 553 + 100*i\n",
    "    light = lgb.train(params, train_data, num_round, feval=evalerror)\n",
    "    light_pred += light.predict(ts)\n",
    "light_pred /= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.hist(light_pred, bins=100);\n",
    "print(pd.Series(np.where(light_pred > 0.51, 1, 0).ravel()).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "train_pool = Pool(tr, label=train_metadata['target'])\n",
    "test_pool = Pool(ts) \n",
    "\n",
    "cat = CatBoostClassifier(random_seed=77)\n",
    "cat.fit(train_pool)\n",
    "cat_pred = cat.predict_proba(test_pool)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.hist(cat_pred, bins=100);\n",
    "print(pd.Series(np.where(cat_pred > 0.51, 1, 0).ravel()).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "dtrain = xgboost.DMatrix(tr, label=train_metadata['target'])\n",
    "dtest = xgboost.DMatrix(ts)\n",
    "params = {\"nthread\": 4, \"seed\": 3, \"subsample\": 0.9, \"reg_lambda\": 11, \"reg_alpha\": 11, \n",
    "          \"learning_rate\": 0.15, \"gamma\": 0, \"colsample_bytree\": 0.8, \n",
    "          \"colsample_bylevel\": 0.9, \"max_depth\": 50, \"objective\": \"binary:logistic\",\n",
    "          'min_child_weight': 33} #hinge\n",
    "num_round = 3000\n",
    "\n",
    "xgb_pred = np.zeros(ts.shape[0])\n",
    "for i in range(1,4):\n",
    "    params['subsample'] = 1.0 - i/10\n",
    "    params['seed'] = i*19\n",
    "    params['learning_rate'] = 0.005 + i/1000 \n",
    "    params['min_child_weight'] = 11*i\n",
    "    xgb = xgboost.train(params, dtrain, num_round)\n",
    "    xgb_pred += xgb.predict(dtest)\n",
    "xgb_pred /= 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.hist(xgb_pred, bins=100);\n",
    "print(pd.Series(np.where(xgb_pred > 0.51, 1, 0).ravel()).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(tr)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='ball_tree', leaf_size=30, \n",
    "                           p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "knn_bg = BaggingClassifier(base_estimator=knn, n_estimators=3, max_samples=0.8, \n",
    "                      max_features=1.0, bootstrap=True, bootstrap_features=False, \n",
    "                      oob_score=False, warm_start=False, n_jobs=1, \n",
    "                      random_state=7, verbose=3)\n",
    "\n",
    "knn_bg.fit(scaler.transform(tr), train_metadata['target'])\n",
    "knn_pred = knn_bg.predict_proba(scaler.transform(ts))[:,1]\n",
    "print(knn_bg.classes_)\n",
    "knn_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.hist(knn_pred, bins=100);\n",
    "print(pd.Series(np.where(knn_pred > 0.51, 1, 0).ravel()).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(146, 146, 146), \n",
    "                   activation=\"relu\", solver=\"adam\", alpha=1e-7, \n",
    "                   batch_size=128, learning_rate=\"constant\", learning_rate_init=0.001, power_t=0.5, \n",
    "                   max_iter=5000, shuffle=True, random_state=2, tol=0.0001, verbose=False, \n",
    "                   warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
    "                   validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
    "\n",
    "nn_pred = np.zeros(ts.shape[0])\n",
    "for i in range(1,4):\n",
    "    nn.set_params(batch_size = 108 + i*10)\n",
    "    nn.set_params(learning_rate_init = 0.0005 + i/2000)\n",
    "    nn.set_params(alpha = i * 1e-7)\n",
    "    nn.fit(scaler.transform(tr), train_metadata['target'])\n",
    "    nn_pred += nn.predict_proba(scaler.transform(ts))[:,1]\n",
    "    print(nn_pred, nn_pred.shape)\n",
    "nn_pred /= 3\n",
    "print(nn.classes_)\n",
    "nn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.hist(nn_pred, bins=100);\n",
    "print(pd.Series(np.where(nn_pred > 0.51, 1, 0).ravel()).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "                        class_weight=None, random_state=11, solver='liblinear', max_iter=1000, \n",
    "                        multi_class='ovr', verbose=0, warm_start=False, n_jobs=4)\n",
    "\n",
    "lr_pred = np.zeros(ts.shape[0])\n",
    "for i in range(1,4):\n",
    "    lr.set_params(random_state = 11 + i)\n",
    "    lr.set_params(C = 1.0 * i)\n",
    "    lr.fit(tr, train_metadata['target'])\n",
    "    lr_pred += lr.predict_proba(ts)[:,1]\n",
    "lr_pred /= 3\n",
    "lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.hist(lr_pred, bins=100);\n",
    "print(pd.Series(np.where(lr_pred > 0.51, 1, 0).ravel()).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb_bg = BaggingClassifier(base_estimator=gnb, n_estimators=3, max_samples=0.8, \n",
    "                      max_features=1.0, bootstrap=True, bootstrap_features=False, \n",
    "                      oob_score=False, warm_start=False, n_jobs=1, \n",
    "                      random_state=7, verbose=3)\n",
    "\n",
    "gnb_bg.fit(tr, train_metadata['target'])\n",
    "gnb_pred = gnb_bg.predict_proba(ts)[:,1]\n",
    "gnb_bg.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.hist(gnb_pred, bins=100);\n",
    "print(pd.Series(np.where(gnb_pred > 0.51, 1, 0).ravel()).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [light_pred, cat_pred, xgb_pred, knn_pred, nn_pred, lr_pred, gnb_pred]\n",
    "weights = [3, 1, 1, 1, 1, 1, 1]\n",
    "pred = np.zeros(ts.shape[0])\n",
    "for i in range(len(preds)):\n",
    "    pred += weights[i] * preds[i]\n",
    "pred = pred / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,17))\n",
    "plt.hist(pred, bins=100);\n",
    "print(pd.Series(np.where(pred > 0.51, 1, 0).ravel()).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.where(pred > 0.51, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"signal_id\": test_metadata['signal_id'],\n",
    "        \"target\": pred\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_feat_importances(feature_names, fi, figsize=(12,8), color=\"blue\"):\n",
    "    feature_importances = fi\n",
    "    feature_importances = pd.Series(\n",
    "        feature_importances, index=feature_names\n",
    "        ).sort_values(ascending=False).iloc[:100]\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.barplot(x=feature_importances, \n",
    "                y=feature_importances.index, \n",
    "                color=color);\n",
    "    plt.xlabel('Feature Importance');\n",
    "    plt.ylabel('Feature');\n",
    "    print(feature_importances.head(25).index)\n",
    "feature_importances = light.feature_importance()\n",
    "feature_names = ts.columns.values\n",
    "plot_feat_importances(feature_names, feature_importances, figsize=(12, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tr.copy()\n",
    "val['target'] = train_metadata['target']\n",
    "val = val.sample(frac=0.7, replace=True, random_state=2)\n",
    "tr_pred = light.predict(val.drop('target', axis=1))\n",
    "tr_pred = np.where(tr_pred > 0.8, 1, 0)\n",
    "mat_cor(val['target'], tr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq.read_metadata('../input/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
