{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale, RobustScaler\n",
    "import gc\n",
    "from skimage.restoration import denoise_wavelet\n",
    "import scipy.signal as signal\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "pd.set_option(\"max_rows\", 200)\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_cor(y_true, y_pred):\n",
    "    assert y_true.shape[0] == y_pred.shape[0]\n",
    "    \n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    numerator = (tp * tn - fp * fn) \n",
    "    denominator = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** .5\n",
    "\n",
    "    return numerator / (denominator + 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    train_metadata = pd.read_csv('../input/metadata_train.csv')\n",
    "    test_metadata = pd.read_csv('../input/metadata_test.csv')\n",
    "    return (train_metadata, test_metadata)\n",
    "\n",
    "def resample_train():\n",
    "    data = pq.read_pandas('../input/train.parquet').to_pandas().transpose()\n",
    "    target = read_metadata()[0]['target'].values\n",
    "    data['target'] = target\n",
    "    p_indices = data[data.target == 0].index\n",
    "    np.random.seed(311)\n",
    "    random_indices = np.random.choice(p_indices, 1777, replace=False)\n",
    "    df = pd.concat([data.loc[random_indices][['target']], \n",
    "                    data[target == 1][['target']]]).sample(frac=1.0, random_state=311)\n",
    "    df.to_csv('train_us_target.csv', index=False)\n",
    "    return df.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_limits = []\n",
    "start = 0\n",
    "end = 8712\n",
    "while True:\n",
    "    if (start+1000) <= 8712:\n",
    "        tr_limits.append((start, start+1000))\n",
    "        start=start+1000\n",
    "    else:\n",
    "        tr_limits.append((start, end))\n",
    "        break\n",
    "        \n",
    "ts_limits = []\n",
    "start = 8712\n",
    "end = 29049\n",
    "while True:\n",
    "    if (start+1000) <= 29049:\n",
    "        ts_limits.append((start, start+1000))\n",
    "        start=start+1000\n",
    "    else:\n",
    "        ts_limits.append((start, end))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata, test_metadata = read_metadata()\n",
    "# resampling_index = resample_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_calc(isTrain, filterSignal=True):\n",
    "    print(\"peak_calc | start:\", time.strftime(\"%H:%M\"))\n",
    "    peak_counts = {}\n",
    "    # peak_counts expected values: [[1,3,5,1,6], [4,5,2,2,1], ..]\n",
    "    \n",
    "    def f1(df):\n",
    "        nonlocal peak_counts\n",
    "        df = df.transpose().values\n",
    "        def filter_sg(sg):\n",
    "            wv = denoise_wavelet(sg, sigma=None, wavelet='haar', mode='hard', \n",
    "                                 wavelet_levels=15, multichannel=False, \n",
    "                                 convert2ycbcr=False, method='VisuShrink')\n",
    "            B, A = signal.butter(N=3, Wn=0.1, output='ba')\n",
    "            smooth_data = signal.filtfilt(B, A, wv)\n",
    "            res = (wv - smooth_data) * 100\n",
    "            return res\n",
    "        \n",
    "        if filterSignal:\n",
    "            df = np.apply_along_axis(filter_sg, 1, df)\n",
    "        \n",
    "        print(\"height\", time.strftime(\"%H:%M\"))\n",
    "        for h in [3, 5, 8, 10, 15, 25, 50, 100]:\n",
    "            peaks = np.apply_along_axis(signal.find_peaks, 1, df, height=h)[:, 0]\n",
    "            num_peaks = [len(x) for x in peaks] \n",
    "            if ('height_more_' + str(h)) in peak_counts:\n",
    "                peak_counts['height_more_' + str(h)].extend(num_peaks)\n",
    "            else:\n",
    "                peak_counts['height_more_' + str(h)] = num_peaks\n",
    "        \n",
    "        print(\"threshold\", time.strftime(\"%H:%M\"))\n",
    "        for t in [3, 10, 16, 50]:\n",
    "            peaks = np.apply_along_axis(signal.find_peaks, 1, df, threshold=t)[:, 0]\n",
    "            num_peaks = [len(x) for x in peaks] \n",
    "            if ('threshold_more_' + str(t)) in peak_counts:\n",
    "                peak_counts['threshold_more_' + str(t)].extend(num_peaks)\n",
    "            else:\n",
    "                peak_counts['threshold_more_' + str(t)] = num_peaks\n",
    "                \n",
    "        print(\"height and distance\", time.strftime(\"%H:%M\"))\n",
    "        for h, d in itertools.product([3, 5, 8, 10, 15, 25, 50, 100],\n",
    "                                      [5, 7, 25, 75, 111, 1000, 11111, 100000]):\n",
    "            peaks = np.apply_along_axis(signal.find_peaks, 1, df, height=h, distance=d)[:, 0]\n",
    "            num_peaks = [len(x) for x in peaks] \n",
    "            col_name = 'h_bt_' + str(h) + '_dist_bt_' + str(d)\n",
    "            if (col_name) in peak_counts:\n",
    "                peak_counts[col_name].extend(num_peaks)\n",
    "            else:\n",
    "                peak_counts[col_name] = num_peaks\n",
    "                \n",
    "        print(\"height and width\", time.strftime(\"%H:%M\"))\n",
    "        for h, w in itertools.product([3, 5, 8, 10, 15, 25, 50, 100],\n",
    "                                      [5, 10, 30, 70, 100, 200, 500]):\n",
    "            peaks = np.apply_along_axis(signal.find_peaks, 1, df, height=h, width=w, rel_height=0.4)[:, 0]\n",
    "            num_peaks = [len(x) for x in peaks] \n",
    "            col_name = 'h_bt_' + str(h) + '_w_bt_' + str(w)\n",
    "            if (col_name) in peak_counts:\n",
    "                peak_counts[col_name].extend(num_peaks)\n",
    "            else:\n",
    "                peak_counts[col_name] = num_peaks\n",
    "        del peaks\n",
    "        \n",
    "        print(\"max/min widths/proms\", time.strftime(\"%H:%M\"))\n",
    "        max_wdth_pk = []\n",
    "        min_wdth_pk = []\n",
    "        max_prom_pk = []\n",
    "        min_prom_pk = []\n",
    "        for n in range(df.shape[0]):\n",
    "            seq = df[n, :]\n",
    "            peaks, _ = signal.find_peaks(seq, height=7, threshold=None, distance=None, width=None)\n",
    "            if len(peaks) == 0:\n",
    "                pk_proms = [[0]]\n",
    "                pk_wdths = [0]\n",
    "            else:\n",
    "                pk_proms = signal.peak_prominences(seq, peaks, wlen=None)\n",
    "                pk_wdths = signal.peak_widths(seq, peaks, rel_height=0.4, \n",
    "                                              prominence_data=pk_proms, wlen=None)[0]\n",
    "            max_wdth_pk.append(max(pk_wdths))\n",
    "            min_wdth_pk.append(min(pk_wdths))\n",
    "            max_prom_pk.append(max(pk_proms[0]))\n",
    "            min_prom_pk.append(min(pk_proms[0]))\n",
    "        if ('max_peak_width') in peak_counts:\n",
    "            peak_counts['max_peak_width'].extend(max_wdth_pk)\n",
    "        else:\n",
    "            peak_counts['max_peak_width'] = max_wdth_pk\n",
    "        if ('min_peak_width') in peak_counts:\n",
    "            peak_counts['min_peak_width'].extend(min_wdth_pk)\n",
    "        else:\n",
    "            peak_counts['min_peak_width'] = min_wdth_pk\n",
    "        if ('max_peak_prom') in peak_counts:\n",
    "            peak_counts['max_peak_prom'].extend(max_prom_pk)\n",
    "        else:\n",
    "            peak_counts['max_peak_prom'] = max_prom_pk\n",
    "        if ('min_peak_prom') in peak_counts:\n",
    "            peak_counts['min_peak_prom'].extend(min_prom_pk)\n",
    "        else:\n",
    "            peak_counts['min_peak_prom'] = min_prom_pk\n",
    "            \n",
    "    if isTrain:\n",
    "        limits = tr_limits\n",
    "        path = '../input/train.parquet'\n",
    "        for i, j in limits:\n",
    "            df = pq.read_pandas(path, columns=[str(i) for i in range(i, j)]).to_pandas()\n",
    "            f1(df)\n",
    "    else:\n",
    "        limits = ts_limits\n",
    "        path = '../input/test.parquet'\n",
    "        for i, j in limits:\n",
    "            df = pq.read_pandas(path, columns=[str(i) for i in range(i, j)]).to_pandas()\n",
    "            f1(df)\n",
    "            \n",
    "    del df; gc.collect()\n",
    "    print(\"peak_calc | end:\", time.strftime(\"%H:%M\"))\n",
    "    return peak_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulse_stats(isTrain):\n",
    "    print(\"pulse_stats | start:\", time.strftime(\"%H:%M\"))\n",
    "    sg_max = np.array([], dtype=np.float16)\n",
    "    sg_min = np.array([], dtype=np.float16)\n",
    "    sg_mean = np.array([], dtype=np.float16)\n",
    "    sg_std = np.array([], dtype=np.float16)\n",
    "    perc1 = np.array([], dtype=np.float16)\n",
    "    perc3 = np.array([], dtype=np.float16)\n",
    "    perc5 = np.array([], dtype=np.float16)\n",
    "    perc7 = np.array([], dtype=np.float16)\n",
    "    perc9 = np.array([], dtype=np.float16)\n",
    "    sg_skew = np.array([], dtype=np.float16)\n",
    "    def f2(df):\n",
    "        nonlocal sg_max, sg_min, sg_mean, sg_std, perc1, perc3, perc5, perc7, perc9, sg_skew\n",
    "        df = df.transpose()\n",
    "        sg_max = np.append(sg_max, df.max(axis=1))\n",
    "        sg_min = np.append(sg_min, df.min(axis=1))\n",
    "        sg_mean = np.append(sg_mean, df.mean(axis=1))\n",
    "        sg_std = np.append(sg_std, df.apply(func=np.std, axis=1, raw=True))\n",
    "        perc1 = np.append(perc1, df.quantile(q=0.1, axis=1))\n",
    "        perc3 = np.append(perc3, df.quantile(q=0.3, axis=1))\n",
    "        perc5 = np.append(perc5, df.quantile(q=0.5, axis=1))\n",
    "        perc7 = np.append(perc7, df.quantile(q=0.7, axis=1))\n",
    "        perc9 = np.append(perc9, df.quantile(q=0.9, axis=1))\n",
    "        sg_skew = np.append(sg_skew, df.apply(func=stats.skew, axis=1, raw=True))\n",
    "\n",
    "    if isTrain:\n",
    "        limits = tr_limits\n",
    "        path = '../input/train.parquet'\n",
    "        for i, j in limits:\n",
    "            df = pq.read_pandas(path, columns=[str(i) for i in range(i, j)]).to_pandas()\n",
    "            f2(df)\n",
    "    else:\n",
    "        limits = ts_limits\n",
    "        path = '../input/test.parquet'\n",
    "        for i, j in limits:\n",
    "            df = pq.read_pandas(path, columns=[str(i) for i in range(i, j)]).to_pandas()\n",
    "            f2(df)\n",
    "    del df; gc.collect()\n",
    "    res = pd.DataFrame({'sg_max': sg_max,\n",
    "                        'sg_min': sg_min,\n",
    "                        'sg_mean': sg_mean,\n",
    "                        'sg_std': sg_std,\n",
    "                        'perc1': perc1,\n",
    "                        'perc3': perc3,\n",
    "                        'perc5': perc5,\n",
    "                        'perc7': perc7,\n",
    "                        'perc9': perc9,\n",
    "                        'sg_skew': sg_skew\n",
    "                       })\n",
    "    print(\"pulse_stats | end:\", time.strftime(\"%H:%M\"))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pulse_stats(isTrain=True)\n",
    "peak_counts = peak_calc(isTrain=True)\n",
    "for k, v in peak_counts.items():\n",
    "    tr[k] = v\n",
    "\n",
    "ts = pulse_stats(isTrain=False)\n",
    "peak_counts = peak_calc(isTrain=False)\n",
    "for k, v in peak_counts.items():\n",
    "    ts[k] = v\n",
    "    \n",
    "del peak_counts; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.to_csv('final_v1_tr', index=False)\n",
    "ts.to_csv('final_v1_ts', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return ('matthews', mat_cor(labels, preds), True)\n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(tr, label=train_metadata['target'])\n",
    "params={'learning_rate': 0.1, 'objective':'binary', 'metric':'None', \n",
    "        'num_leaves': 777, 'verbose': 1, 'random_state':311, 'max_depth': 11,\n",
    "        'bagging_fraction': 0.7, 'feature_fraction': 1.0, 'min_data_in_leaf': 33,\n",
    "        'is_unbalance': True}\n",
    "num_round = 15000\n",
    "light = lgb.train(params, train_data, num_round, feval=evalerror)\n",
    "pred = light.predict(ts)\n",
    "feature_importances = light.feature_importance()\n",
    "feature_names = ts.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.where(pred > 0.6, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"signal_id\": test_metadata['signal_id'],\n",
    "        \"target\": pred\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_feat_importances(feature_names, fi, figsize=(12,8), color=\"royalblue\"):\n",
    "    feature_importances = fi\n",
    "    feature_importances = pd.Series(\n",
    "        feature_importances, index=feature_names\n",
    "        ).sort_values(ascending=False).iloc[:100]\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.barplot(x=feature_importances, \n",
    "                y=feature_importances.index, \n",
    "                color=color);\n",
    "    plt.xlabel('Feature Importance');\n",
    "    plt.ylabel('Feature');\n",
    "\n",
    "plot_feat_importances(feature_names, feature_importances, figsize=(12, 30))\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tr.copy()\n",
    "val['target'] = pd.read_csv('train_us_target.csv')\n",
    "val = val.sample(frac=0.7, replace=True, random_state=2)\n",
    "tr_pred = light.predict(val.drop('target', axis=1))\n",
    "tr_pred = np.where(tr_pred > 0.8, 1, 0)\n",
    "mat_cor(val['target'], tr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq.read_metadata('../input/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
